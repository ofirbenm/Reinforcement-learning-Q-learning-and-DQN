{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning: Q-learning and DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **First project - Reinforcement Learning**"
      ],
      "metadata": {
        "id": "np57YMB54ABC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment i will get to know and experiment with OpenAI Gym, a testbed for reinforcement learning algorithms containing environments in different difficulty levels. \\\n",
        "I will implement 2 algorithms: \\\n",
        "1. tabular Q-learning model on a simple environment.\n",
        "2. Neural Network function approximator of the Q-value, using the basic DQN algorithm."
      ],
      "metadata": {
        "id": "hKQ8GsJx3Sm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1 - Q-learning Algorithm\n"
      ],
      "metadata": {
        "id": "fDHisLxa3HW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Hvy-2iN4gJFz"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Q-Learning Agent**\n",
        "The Q-learning agent class will have two functions: \\\n",
        "**action** - The agent selects the action with the highest value from whatever state it is in (exploitation), and with some probability chooses a new action (exploration). \\\n",
        "**update_QL** - Update the transitions probability matrix based on the Q-function."
      ],
      "metadata": {
        "id": "BCoDtEw3lFWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QL_Agent:\n",
        "  def __init__(self, env, discount_factor, learning_rate, epsilon):\n",
        "    self.g = discount_factor\n",
        "    self.lr = learning_rate\n",
        "    self.epsilon = epsilon\n",
        "    self.env = env\n",
        "    self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "  \n",
        "  def action(self, s):\n",
        "    rand = random.uniform(0,1)\n",
        "    if rand > self.epsilon:\n",
        "      action = np.argmax(self.q_table[s,:])\n",
        "    else:\n",
        "      action = self.env.action_space.sample()\n",
        "    return action\n",
        "  \n",
        "  def update_QL(self, state, action, reward, state_):\n",
        "    self.q_table[state, action] = self.q_table[state, action] * (1- self.lr) + \\\n",
        "    self.lr * (reward + self.g * np.max(self.q_table[state_, :]))"
      ],
      "metadata": {
        "id": "ElXJn3cIg13K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base Loop** - train the model"
      ],
      "metadata": {
        "id": "yL_Tswj9nTSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "# Exporation parameters\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.1\n",
        "decay_rate = 0.0005\n",
        "agentQL = QL_Agent(env, discount_factor = 0.95, learning_rate = 0.1, epsilon = max_epsilon)\n",
        "\n",
        "total_episodes = 50000\n",
        "rewards = np.zeros((total_episodes))\n",
        "for i in range(total_episodes):\n",
        "  total_rewards = 0\n",
        "  state = env.reset() #Initialize the Frozen-Lake MDP\n",
        "  while True: # Full trajectory\n",
        "    action = agentQL.action(state) # Apply epsilon-Greedy policy\n",
        "    state_, reward, done, _ = env.step(action) # Observe the next state and rewrd\n",
        "\n",
        "    agentQL.update_QL(state, action, reward, state_) # Value function update\n",
        "\n",
        "    state = state_ # Next state\n",
        "\n",
        "    total_rewards += reward\n",
        "\n",
        "    # Adjust exploration rate\n",
        "    agentQL.epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*i)\n",
        "    if done:  # The agent reached to Hole or Goal step\n",
        "      break\n",
        "  \n",
        "  rewards[i] = total_rewards"
      ],
      "metadata": {
        "id": "FCL5OAKxhvrI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-table after training"
      ],
      "metadata": {
        "id": "Unykel4UvP3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(agentQL.q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCzAybcahRxK",
        "outputId": "8d7d705c-2d89-488e-ee85-2f45522c3fc6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.18307622 0.13974704 0.15696406 0.14920029]\n",
            " [0.07412679 0.10290819 0.0816559  0.1288266 ]\n",
            " [0.12448889 0.1176815  0.11536705 0.11814416]\n",
            " [0.07051886 0.0774682  0.07091989 0.10667671]\n",
            " [0.24607347 0.14364659 0.13595893 0.10287033]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.15742876 0.07468276 0.12095522 0.04590568]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.15379458 0.19556729 0.14237735 0.28247631]\n",
            " [0.25715105 0.38964815 0.18201849 0.19872703]\n",
            " [0.32397831 0.27137658 0.24956924 0.17362351]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.28907252 0.3017422  0.57138967 0.37253722]\n",
            " [0.52111572 0.5627462  0.75900611 0.53094951]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate our Q-learning agent after training\n",
        "Once we've finished the agent's training, let's see how well he's learned to get to the Goal-state."
      ],
      "metadata": {
        "id": "TAIP6B1HwZoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "total_episodes = 300\n",
        "rewards = np.zeros((total_episodes))\n",
        "agentQL.epsilon = 0.0 # Greedy policy\n",
        "\n",
        "for i in range(total_episodes):\n",
        "  total_rewards = 0\n",
        "  state = env.reset() #Initialize the Frozen-Lake MDP\n",
        "  while True: # Full trajectory\n",
        "    action = agentQL.action(state) # Apply epsilon-Greedy policy\n",
        "    state_, reward, done, _ = env.step(action) # Observe the next state and rewrd\n",
        "\n",
        "    # agentQL.update_QL(state, action, reward, state_) # Value function update\n",
        "\n",
        "    state = state_ # Next state\n",
        "\n",
        "    total_rewards += reward\n",
        "\n",
        "    if done:  # The agent reached to Hole or Goal step\n",
        "      env.render()\n",
        "      break\n",
        "  \n",
        "  rewards[i] = total_rewards\n",
        "  print(\"Evaluation Episode: \" + str(i) + \" --> Reward: \" + str(rewards[i]))\n",
        "  print(\"Mean Reward: \" + str(np.sum(rewards / total_episodes)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezuJ7iXXwhdK",
        "outputId": "f63b7ac7-f0d4-4054-9e68-159ae942fad5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 0 --> Reward: 0.0\n",
            "Mean Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 1 --> Reward: 0.0\n",
            "Mean Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 2 --> Reward: 0.0\n",
            "Mean Reward: 0.0\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 3 --> Reward: 0.0\n",
            "Mean Reward: 0.0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 4 --> Reward: 1.0\n",
            "Mean Reward: 0.0033333333333333335\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 5 --> Reward: 0.0\n",
            "Mean Reward: 0.0033333333333333335\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 6 --> Reward: 1.0\n",
            "Mean Reward: 0.006666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 7 --> Reward: 0.0\n",
            "Mean Reward: 0.006666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 8 --> Reward: 1.0\n",
            "Mean Reward: 0.01\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 9 --> Reward: 1.0\n",
            "Mean Reward: 0.013333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 10 --> Reward: 1.0\n",
            "Mean Reward: 0.016666666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 11 --> Reward: 0.0\n",
            "Mean Reward: 0.016666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 12 --> Reward: 1.0\n",
            "Mean Reward: 0.02\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 13 --> Reward: 1.0\n",
            "Mean Reward: 0.023333333333333334\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 14 --> Reward: 0.0\n",
            "Mean Reward: 0.023333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 15 --> Reward: 1.0\n",
            "Mean Reward: 0.026666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 16 --> Reward: 1.0\n",
            "Mean Reward: 0.03\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 17 --> Reward: 1.0\n",
            "Mean Reward: 0.03333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 18 --> Reward: 1.0\n",
            "Mean Reward: 0.03666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 19 --> Reward: 0.0\n",
            "Mean Reward: 0.03666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 20 --> Reward: 0.0\n",
            "Mean Reward: 0.03666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 21 --> Reward: 0.0\n",
            "Mean Reward: 0.03666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 22 --> Reward: 1.0\n",
            "Mean Reward: 0.04\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 23 --> Reward: 0.0\n",
            "Mean Reward: 0.04\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 24 --> Reward: 1.0\n",
            "Mean Reward: 0.043333333333333335\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 25 --> Reward: 0.0\n",
            "Mean Reward: 0.043333333333333335\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 26 --> Reward: 1.0\n",
            "Mean Reward: 0.04666666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 27 --> Reward: 0.0\n",
            "Mean Reward: 0.04666666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 28 --> Reward: 0.0\n",
            "Mean Reward: 0.04666666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 29 --> Reward: 0.0\n",
            "Mean Reward: 0.04666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 30 --> Reward: 1.0\n",
            "Mean Reward: 0.05\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Evaluation Episode: 31 --> Reward: 0.0\n",
            "Mean Reward: 0.05\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 32 --> Reward: 0.0\n",
            "Mean Reward: 0.05\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 33 --> Reward: 0.0\n",
            "Mean Reward: 0.05\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 34 --> Reward: 0.0\n",
            "Mean Reward: 0.05\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 35 --> Reward: 1.0\n",
            "Mean Reward: 0.05333333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 36 --> Reward: 0.0\n",
            "Mean Reward: 0.05333333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 37 --> Reward: 0.0\n",
            "Mean Reward: 0.05333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 38 --> Reward: 1.0\n",
            "Mean Reward: 0.056666666666666664\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 39 --> Reward: 1.0\n",
            "Mean Reward: 0.06\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 40 --> Reward: 0.0\n",
            "Mean Reward: 0.06\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 41 --> Reward: 1.0\n",
            "Mean Reward: 0.06333333333333332\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 42 --> Reward: 1.0\n",
            "Mean Reward: 0.06666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 43 --> Reward: 0.0\n",
            "Mean Reward: 0.06666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 44 --> Reward: 1.0\n",
            "Mean Reward: 0.07\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 45 --> Reward: 1.0\n",
            "Mean Reward: 0.07333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 46 --> Reward: 1.0\n",
            "Mean Reward: 0.07666666666666666\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 47 --> Reward: 0.0\n",
            "Mean Reward: 0.07666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 48 --> Reward: 1.0\n",
            "Mean Reward: 0.08\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 49 --> Reward: 1.0\n",
            "Mean Reward: 0.08333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 50 --> Reward: 1.0\n",
            "Mean Reward: 0.08666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 51 --> Reward: 1.0\n",
            "Mean Reward: 0.09\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 52 --> Reward: 1.0\n",
            "Mean Reward: 0.09333333333333334\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 53 --> Reward: 0.0\n",
            "Mean Reward: 0.09333333333333334\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 54 --> Reward: 0.0\n",
            "Mean Reward: 0.09333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 55 --> Reward: 1.0\n",
            "Mean Reward: 0.09666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 56 --> Reward: 1.0\n",
            "Mean Reward: 0.09999999999999999\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 57 --> Reward: 1.0\n",
            "Mean Reward: 0.10333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 58 --> Reward: 1.0\n",
            "Mean Reward: 0.10666666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 59 --> Reward: 0.0\n",
            "Mean Reward: 0.10666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 60 --> Reward: 1.0\n",
            "Mean Reward: 0.11\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 61 --> Reward: 1.0\n",
            "Mean Reward: 0.11333333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 62 --> Reward: 0.0\n",
            "Mean Reward: 0.11333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 63 --> Reward: 1.0\n",
            "Mean Reward: 0.11666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 64 --> Reward: 1.0\n",
            "Mean Reward: 0.12\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 65 --> Reward: 1.0\n",
            "Mean Reward: 0.12333333333333332\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 66 --> Reward: 1.0\n",
            "Mean Reward: 0.12666666666666668\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 67 --> Reward: 1.0\n",
            "Mean Reward: 0.13\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 68 --> Reward: 1.0\n",
            "Mean Reward: 0.13333333333333333\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 69 --> Reward: 0.0\n",
            "Mean Reward: 0.13333333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 70 --> Reward: 0.0\n",
            "Mean Reward: 0.13333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 71 --> Reward: 1.0\n",
            "Mean Reward: 0.13666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 72 --> Reward: 1.0\n",
            "Mean Reward: 0.13999999999999999\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 73 --> Reward: 1.0\n",
            "Mean Reward: 0.1433333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 74 --> Reward: 1.0\n",
            "Mean Reward: 0.14666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 75 --> Reward: 1.0\n",
            "Mean Reward: 0.15\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 76 --> Reward: 1.0\n",
            "Mean Reward: 0.15333333333333332\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 77 --> Reward: 1.0\n",
            "Mean Reward: 0.15666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 78 --> Reward: 1.0\n",
            "Mean Reward: 0.16\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 79 --> Reward: 0.0\n",
            "Mean Reward: 0.16\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 80 --> Reward: 1.0\n",
            "Mean Reward: 0.16333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 81 --> Reward: 1.0\n",
            "Mean Reward: 0.16666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 82 --> Reward: 1.0\n",
            "Mean Reward: 0.16999999999999998\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 83 --> Reward: 0.0\n",
            "Mean Reward: 0.16999999999999998\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 84 --> Reward: 1.0\n",
            "Mean Reward: 0.17333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 85 --> Reward: 1.0\n",
            "Mean Reward: 0.17666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 86 --> Reward: 0.0\n",
            "Mean Reward: 0.17666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 87 --> Reward: 0.0\n",
            "Mean Reward: 0.17666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 88 --> Reward: 1.0\n",
            "Mean Reward: 0.18\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 89 --> Reward: 0.0\n",
            "Mean Reward: 0.18\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 90 --> Reward: 1.0\n",
            "Mean Reward: 0.18333333333333332\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 91 --> Reward: 1.0\n",
            "Mean Reward: 0.18666666666666665\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 92 --> Reward: 0.0\n",
            "Mean Reward: 0.18666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 93 --> Reward: 1.0\n",
            "Mean Reward: 0.19\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 94 --> Reward: 1.0\n",
            "Mean Reward: 0.19333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 95 --> Reward: 1.0\n",
            "Mean Reward: 0.19666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 96 --> Reward: 1.0\n",
            "Mean Reward: 0.19999999999999998\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 97 --> Reward: 1.0\n",
            "Mean Reward: 0.2033333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 98 --> Reward: 1.0\n",
            "Mean Reward: 0.20666666666666667\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "Evaluation Episode: 99 --> Reward: 0.0\n",
            "Mean Reward: 0.20666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 100 --> Reward: 1.0\n",
            "Mean Reward: 0.21\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 101 --> Reward: 0.0\n",
            "Mean Reward: 0.21\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 102 --> Reward: 1.0\n",
            "Mean Reward: 0.21333333333333332\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 103 --> Reward: 1.0\n",
            "Mean Reward: 0.21666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 104 --> Reward: 1.0\n",
            "Mean Reward: 0.21999999999999997\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 105 --> Reward: 0.0\n",
            "Mean Reward: 0.21999999999999997\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 106 --> Reward: 0.0\n",
            "Mean Reward: 0.21999999999999997\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 107 --> Reward: 1.0\n",
            "Mean Reward: 0.22333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 108 --> Reward: 1.0\n",
            "Mean Reward: 0.22666666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 109 --> Reward: 0.0\n",
            "Mean Reward: 0.22666666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 110 --> Reward: 0.0\n",
            "Mean Reward: 0.22666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 111 --> Reward: 1.0\n",
            "Mean Reward: 0.22999999999999998\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 112 --> Reward: 1.0\n",
            "Mean Reward: 0.23333333333333334\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 113 --> Reward: 0.0\n",
            "Mean Reward: 0.23333333333333334\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 114 --> Reward: 0.0\n",
            "Mean Reward: 0.23333333333333334\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 115 --> Reward: 0.0\n",
            "Mean Reward: 0.23333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 116 --> Reward: 1.0\n",
            "Mean Reward: 0.23666666666666664\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 117 --> Reward: 1.0\n",
            "Mean Reward: 0.24\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 118 --> Reward: 0.0\n",
            "Mean Reward: 0.24\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 119 --> Reward: 1.0\n",
            "Mean Reward: 0.24333333333333332\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 120 --> Reward: 1.0\n",
            "Mean Reward: 0.24666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 121 --> Reward: 1.0\n",
            "Mean Reward: 0.25\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 122 --> Reward: 1.0\n",
            "Mean Reward: 0.2533333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 123 --> Reward: 1.0\n",
            "Mean Reward: 0.25666666666666665\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 124 --> Reward: 0.0\n",
            "Mean Reward: 0.25666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 125 --> Reward: 1.0\n",
            "Mean Reward: 0.26\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 126 --> Reward: 1.0\n",
            "Mean Reward: 0.2633333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 127 --> Reward: 0.0\n",
            "Mean Reward: 0.2633333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 128 --> Reward: 1.0\n",
            "Mean Reward: 0.26666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 129 --> Reward: 1.0\n",
            "Mean Reward: 0.27\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 130 --> Reward: 1.0\n",
            "Mean Reward: 0.2733333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 131 --> Reward: 1.0\n",
            "Mean Reward: 0.27666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 132 --> Reward: 0.0\n",
            "Mean Reward: 0.27666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 133 --> Reward: 0.0\n",
            "Mean Reward: 0.27666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 134 --> Reward: 0.0\n",
            "Mean Reward: 0.27666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 135 --> Reward: 1.0\n",
            "Mean Reward: 0.27999999999999997\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 136 --> Reward: 1.0\n",
            "Mean Reward: 0.2833333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 137 --> Reward: 1.0\n",
            "Mean Reward: 0.2866666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 138 --> Reward: 1.0\n",
            "Mean Reward: 0.29\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 139 --> Reward: 0.0\n",
            "Mean Reward: 0.29\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 140 --> Reward: 0.0\n",
            "Mean Reward: 0.29\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 141 --> Reward: 1.0\n",
            "Mean Reward: 0.29333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 142 --> Reward: 1.0\n",
            "Mean Reward: 0.29666666666666663\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 143 --> Reward: 1.0\n",
            "Mean Reward: 0.3\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 144 --> Reward: 1.0\n",
            "Mean Reward: 0.30333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 145 --> Reward: 1.0\n",
            "Mean Reward: 0.30666666666666664\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 146 --> Reward: 0.0\n",
            "Mean Reward: 0.30666666666666664\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 147 --> Reward: 1.0\n",
            "Mean Reward: 0.31\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 148 --> Reward: 0.0\n",
            "Mean Reward: 0.31\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 149 --> Reward: 1.0\n",
            "Mean Reward: 0.3133333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 150 --> Reward: 1.0\n",
            "Mean Reward: 0.31666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 151 --> Reward: 1.0\n",
            "Mean Reward: 0.32\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 152 --> Reward: 1.0\n",
            "Mean Reward: 0.3233333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 153 --> Reward: 0.0\n",
            "Mean Reward: 0.3233333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 154 --> Reward: 0.0\n",
            "Mean Reward: 0.3233333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 155 --> Reward: 1.0\n",
            "Mean Reward: 0.32666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 156 --> Reward: 1.0\n",
            "Mean Reward: 0.32999999999999996\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 157 --> Reward: 1.0\n",
            "Mean Reward: 0.3333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 158 --> Reward: 1.0\n",
            "Mean Reward: 0.33666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 159 --> Reward: 1.0\n",
            "Mean Reward: 0.33999999999999997\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 160 --> Reward: 0.0\n",
            "Mean Reward: 0.33999999999999997\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 161 --> Reward: 1.0\n",
            "Mean Reward: 0.3433333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 162 --> Reward: 0.0\n",
            "Mean Reward: 0.3433333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 163 --> Reward: 1.0\n",
            "Mean Reward: 0.3466666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 164 --> Reward: 1.0\n",
            "Mean Reward: 0.35\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 165 --> Reward: 1.0\n",
            "Mean Reward: 0.35333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 166 --> Reward: 1.0\n",
            "Mean Reward: 0.3566666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 167 --> Reward: 0.0\n",
            "Mean Reward: 0.3566666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 168 --> Reward: 0.0\n",
            "Mean Reward: 0.3566666666666667\n",
            "  (Left)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 169 --> Reward: 0.0\n",
            "Mean Reward: 0.3566666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 170 --> Reward: 1.0\n",
            "Mean Reward: 0.36\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 171 --> Reward: 1.0\n",
            "Mean Reward: 0.3633333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 172 --> Reward: 1.0\n",
            "Mean Reward: 0.36666666666666664\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 173 --> Reward: 0.0\n",
            "Mean Reward: 0.36666666666666664\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 174 --> Reward: 1.0\n",
            "Mean Reward: 0.37\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 175 --> Reward: 1.0\n",
            "Mean Reward: 0.3733333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 176 --> Reward: 0.0\n",
            "Mean Reward: 0.3733333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 177 --> Reward: 0.0\n",
            "Mean Reward: 0.3733333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 178 --> Reward: 1.0\n",
            "Mean Reward: 0.37666666666666665\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 179 --> Reward: 0.0\n",
            "Mean Reward: 0.37666666666666665\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 180 --> Reward: 0.0\n",
            "Mean Reward: 0.37666666666666665\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 181 --> Reward: 0.0\n",
            "Mean Reward: 0.37666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 182 --> Reward: 1.0\n",
            "Mean Reward: 0.38\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 183 --> Reward: 1.0\n",
            "Mean Reward: 0.3833333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 184 --> Reward: 1.0\n",
            "Mean Reward: 0.38666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 185 --> Reward: 1.0\n",
            "Mean Reward: 0.39\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 186 --> Reward: 1.0\n",
            "Mean Reward: 0.3933333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 187 --> Reward: 0.0\n",
            "Mean Reward: 0.3933333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 188 --> Reward: 0.0\n",
            "Mean Reward: 0.3933333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 189 --> Reward: 0.0\n",
            "Mean Reward: 0.3933333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 190 --> Reward: 0.0\n",
            "Mean Reward: 0.3933333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 191 --> Reward: 1.0\n",
            "Mean Reward: 0.39666666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 192 --> Reward: 0.0\n",
            "Mean Reward: 0.39666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 193 --> Reward: 1.0\n",
            "Mean Reward: 0.4\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 194 --> Reward: 0.0\n",
            "Mean Reward: 0.4\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 195 --> Reward: 1.0\n",
            "Mean Reward: 0.4033333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 196 --> Reward: 1.0\n",
            "Mean Reward: 0.4066666666666667\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 197 --> Reward: 0.0\n",
            "Mean Reward: 0.4066666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 198 --> Reward: 1.0\n",
            "Mean Reward: 0.41\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 199 --> Reward: 0.0\n",
            "Mean Reward: 0.41\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 200 --> Reward: 1.0\n",
            "Mean Reward: 0.41333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 201 --> Reward: 1.0\n",
            "Mean Reward: 0.41666666666666663\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 202 --> Reward: 0.0\n",
            "Mean Reward: 0.41666666666666663\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 203 --> Reward: 1.0\n",
            "Mean Reward: 0.42\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 204 --> Reward: 0.0\n",
            "Mean Reward: 0.42\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 205 --> Reward: 0.0\n",
            "Mean Reward: 0.42\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 206 --> Reward: 1.0\n",
            "Mean Reward: 0.42333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 207 --> Reward: 1.0\n",
            "Mean Reward: 0.42666666666666664\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 208 --> Reward: 1.0\n",
            "Mean Reward: 0.43\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 209 --> Reward: 0.0\n",
            "Mean Reward: 0.43\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 210 --> Reward: 1.0\n",
            "Mean Reward: 0.43333333333333335\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 211 --> Reward: 1.0\n",
            "Mean Reward: 0.43666666666666665\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 212 --> Reward: 1.0\n",
            "Mean Reward: 0.44\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 213 --> Reward: 1.0\n",
            "Mean Reward: 0.44333333333333336\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 214 --> Reward: 1.0\n",
            "Mean Reward: 0.44666666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 215 --> Reward: 1.0\n",
            "Mean Reward: 0.45\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 216 --> Reward: 1.0\n",
            "Mean Reward: 0.45333333333333337\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 217 --> Reward: 1.0\n",
            "Mean Reward: 0.45666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 218 --> Reward: 1.0\n",
            "Mean Reward: 0.46\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 219 --> Reward: 1.0\n",
            "Mean Reward: 0.4633333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 220 --> Reward: 1.0\n",
            "Mean Reward: 0.4666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 221 --> Reward: 1.0\n",
            "Mean Reward: 0.47\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 222 --> Reward: 1.0\n",
            "Mean Reward: 0.4733333333333334\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 223 --> Reward: 0.0\n",
            "Mean Reward: 0.4733333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 224 --> Reward: 1.0\n",
            "Mean Reward: 0.4766666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 225 --> Reward: 1.0\n",
            "Mean Reward: 0.48\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 226 --> Reward: 0.0\n",
            "Mean Reward: 0.48\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 227 --> Reward: 0.0\n",
            "Mean Reward: 0.48\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 228 --> Reward: 1.0\n",
            "Mean Reward: 0.48333333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 229 --> Reward: 1.0\n",
            "Mean Reward: 0.4866666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 230 --> Reward: 0.0\n",
            "Mean Reward: 0.4866666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 231 --> Reward: 1.0\n",
            "Mean Reward: 0.49\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 232 --> Reward: 1.0\n",
            "Mean Reward: 0.49333333333333335\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 233 --> Reward: 1.0\n",
            "Mean Reward: 0.4966666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 234 --> Reward: 1.0\n",
            "Mean Reward: 0.5\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 235 --> Reward: 1.0\n",
            "Mean Reward: 0.5033333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 236 --> Reward: 1.0\n",
            "Mean Reward: 0.5066666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 237 --> Reward: 1.0\n",
            "Mean Reward: 0.51\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 238 --> Reward: 1.0\n",
            "Mean Reward: 0.5133333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 239 --> Reward: 1.0\n",
            "Mean Reward: 0.5166666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 240 --> Reward: 0.0\n",
            "Mean Reward: 0.5166666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 241 --> Reward: 0.0\n",
            "Mean Reward: 0.5166666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 242 --> Reward: 1.0\n",
            "Mean Reward: 0.52\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 243 --> Reward: 0.0\n",
            "Mean Reward: 0.52\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 244 --> Reward: 1.0\n",
            "Mean Reward: 0.5233333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 245 --> Reward: 0.0\n",
            "Mean Reward: 0.5233333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 246 --> Reward: 1.0\n",
            "Mean Reward: 0.5266666666666666\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 247 --> Reward: 0.0\n",
            "Mean Reward: 0.5266666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 248 --> Reward: 1.0\n",
            "Mean Reward: 0.53\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 249 --> Reward: 1.0\n",
            "Mean Reward: 0.5333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 250 --> Reward: 1.0\n",
            "Mean Reward: 0.5366666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 251 --> Reward: 0.0\n",
            "Mean Reward: 0.5366666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 252 --> Reward: 0.0\n",
            "Mean Reward: 0.5366666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 253 --> Reward: 0.0\n",
            "Mean Reward: 0.5366666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 254 --> Reward: 1.0\n",
            "Mean Reward: 0.54\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 255 --> Reward: 1.0\n",
            "Mean Reward: 0.5433333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 256 --> Reward: 1.0\n",
            "Mean Reward: 0.5466666666666666\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 257 --> Reward: 0.0\n",
            "Mean Reward: 0.5466666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 258 --> Reward: 1.0\n",
            "Mean Reward: 0.55\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 259 --> Reward: 1.0\n",
            "Mean Reward: 0.5533333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 260 --> Reward: 1.0\n",
            "Mean Reward: 0.5566666666666666\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 261 --> Reward: 1.0\n",
            "Mean Reward: 0.56\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 262 --> Reward: 1.0\n",
            "Mean Reward: 0.5633333333333334\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 263 --> Reward: 1.0\n",
            "Mean Reward: 0.5666666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 264 --> Reward: 1.0\n",
            "Mean Reward: 0.5700000000000001\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 265 --> Reward: 0.0\n",
            "Mean Reward: 0.5700000000000001\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 266 --> Reward: 1.0\n",
            "Mean Reward: 0.5733333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 267 --> Reward: 1.0\n",
            "Mean Reward: 0.5766666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 268 --> Reward: 0.0\n",
            "Mean Reward: 0.5766666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 269 --> Reward: 0.0\n",
            "Mean Reward: 0.5766666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 270 --> Reward: 0.0\n",
            "Mean Reward: 0.5766666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 271 --> Reward: 1.0\n",
            "Mean Reward: 0.5800000000000001\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 272 --> Reward: 1.0\n",
            "Mean Reward: 0.5833333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 273 --> Reward: 0.0\n",
            "Mean Reward: 0.5833333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 274 --> Reward: 1.0\n",
            "Mean Reward: 0.5866666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 275 --> Reward: 0.0\n",
            "Mean Reward: 0.5866666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 276 --> Reward: 1.0\n",
            "Mean Reward: 0.5900000000000001\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 277 --> Reward: 0.0\n",
            "Mean Reward: 0.5900000000000001\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 278 --> Reward: 1.0\n",
            "Mean Reward: 0.5933333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 279 --> Reward: 1.0\n",
            "Mean Reward: 0.5966666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 280 --> Reward: 1.0\n",
            "Mean Reward: 0.6000000000000001\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 281 --> Reward: 1.0\n",
            "Mean Reward: 0.6033333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 282 --> Reward: 0.0\n",
            "Mean Reward: 0.6033333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 283 --> Reward: 1.0\n",
            "Mean Reward: 0.6066666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 284 --> Reward: 1.0\n",
            "Mean Reward: 0.6100000000000001\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 285 --> Reward: 1.0\n",
            "Mean Reward: 0.6133333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 286 --> Reward: 1.0\n",
            "Mean Reward: 0.6166666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 287 --> Reward: 1.0\n",
            "Mean Reward: 0.6200000000000001\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 288 --> Reward: 1.0\n",
            "Mean Reward: 0.6233333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 289 --> Reward: 1.0\n",
            "Mean Reward: 0.6266666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 290 --> Reward: 1.0\n",
            "Mean Reward: 0.63\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 291 --> Reward: 1.0\n",
            "Mean Reward: 0.6333333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 292 --> Reward: 0.0\n",
            "Mean Reward: 0.6333333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 293 --> Reward: 0.0\n",
            "Mean Reward: 0.6333333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 294 --> Reward: 1.0\n",
            "Mean Reward: 0.6366666666666667\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 295 --> Reward: 0.0\n",
            "Mean Reward: 0.6366666666666667\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 296 --> Reward: 1.0\n",
            "Mean Reward: 0.64\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 297 --> Reward: 1.0\n",
            "Mean Reward: 0.6433333333333333\n",
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Evaluation Episode: 298 --> Reward: 0.0\n",
            "Mean Reward: 0.6433333333333333\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Evaluation Episode: 299 --> Reward: 1.0\n",
            "Mean Reward: 0.6466666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, it can be seen that the agent's probability of getting from the initial state to the final state is around 64%, this can be understood for the reason that the environment is stochastic."
      ],
      "metadata": {
        "id": "N1mt3o4i2iob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2 - DQN algorithm"
      ],
      "metadata": {
        "id": "KVtPfRk74Q5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "DPDc5bow2cuK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network class\n",
        "1. 5 hidden layers\n",
        "2. relu activation function\n",
        "3. MSE loss function using ADAM optimizer"
      ],
      "metadata": {
        "id": "BTo1mD9g9pgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_network(lr, net_arch):\n",
        "    \"\"\"\n",
        "    Build NN\n",
        "\n",
        "    :param lr: Learning rate\n",
        "    :param net_arch: Network architecture\n",
        "    :return: NN - Get state as input and return Q(s,a)\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=64, activation='relu', input_dim=net_arch['state_dim'], kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(units=24, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(units=24, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(units=net_arch['action_dim'], activation='linear', kernel_initializer='he_uniform'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "_TAuY4QA9hhb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Buffer Class\n",
        "use to store and sample random batches of experiences for training the net.\\\n",
        "It is used in the algorithm to sample a minibatch of experiences in random order for the optimization process\n"
      ],
      "metadata": {
        "id": "11VPfyig_Yg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        :param capacity: The number of experience batches to hold\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def store(self, experience):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory[self.position % self.capacity] = experience\n",
        "        self.position += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        mini_batch = random.sample(self.memory, batch_size)\n",
        "        states = np.array([exp[0] for exp in mini_batch])\n",
        "        actions = np.array([exp[1] for exp in mini_batch])\n",
        "        next_states = np.array([exp[2] for exp in mini_batch])\n",
        "        rewards = np.array([exp[3] for exp in mini_batch])\n",
        "        not_dones = np.array([exp[4] for exp in mini_batch])\n",
        "        return states, actions, next_states, rewards, not_dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "2p7nXxlW9lrj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN_Agent Class - \n",
        "Will contain two neural networks - target and online. \\\n",
        "The online network is the network that actually practices and performs the weaving. \\\n",
        "The target network is the network that we freeze for a number of training iterations to avoid a non-stationary target situation."
      ],
      "metadata": {
        "id": "_nxndmTYAHfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN_Agent:\n",
        "    def __init__(self, hp, net_arch):\n",
        "        \"\"\"\n",
        "        The DQN agent class. use to store experience, choose action, decaying epsilon, update the online network\n",
        "        and test the agent\n",
        "\n",
        "        :param hp: Hyper-parameters\n",
        "        :param env_dim: Environment dimensions\n",
        "        \"\"\"\n",
        "        self.hp = hp\n",
        "        self.net_arch = net_arch\n",
        "        self.epsilon = self.hp['max_epsilon']\n",
        "        self.memory = ReplayBuffer(capacity=self.hp['capacity'])\n",
        "        self.online_net = build_network(lr=self.hp['lr'], net_arch=self.net_arch)\n",
        "        self.target_net = build_network(lr=self.hp['lr'], net_arch=self.net_arch)\n",
        "        self.target_net.set_weights(self.online_net.get_weights())\n",
        "\n",
        "    def store_experience(self, experience):\n",
        "        self.memory.store(experience=experience)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            a = np.random.choice(self.net_arch['action_dim'])  # Explore\n",
        "        else:\n",
        "            a = np.argmax(self.online_net.predict(x=np.array([observation])))  # Exploit\n",
        "        self.epsilon_decay()\n",
        "        return a\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        self.epsilon = max(self.hp['min_epsilon'], self.epsilon * self.hp['epsilon_decay'])\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.set_weights(self.online_net.get_weights())\n",
        "\n",
        "    def update_step(self):\n",
        "        self.epsilon_decay()\n",
        "        if len(self.memory) < self.hp['batch_size']: return 0\n",
        "\n",
        "        # Sample minibatch\n",
        "        states, actions, next_states, rewards, not_dones = self.memory.sample(batch_size=self.hp['batch_size'])\n",
        "\n",
        "        predicted_q = self.online_net.predict(states)  # Q(states_batch,:) from the online network\n",
        "        target_q_next = self.target_net.predict(next_states)  # Q(next_states_batch,:) from the target network\n",
        "\n",
        "        q_target = np.copy(predicted_q)  # Copy of Q(s,:) from the online network\n",
        "        batch_index = np.arange(self.hp['batch_size'])\n",
        "\n",
        "        # Q target = Q(s,a) = r + gamma * MAX[Q_target_net(s',A')] * not_dones\n",
        "        q_target[batch_index, actions] = rewards + self.hp['gamma'] * np.amax(target_q_next, axis=1) * not_dones\n",
        "        \n",
        "        # Update online network to give q values as output from the suit states inputs\n",
        "        loss = self.online_net.train_on_batch(states, q_target)\n",
        "        return loss\n",
        "\n",
        "    def test_agent(self, env, visualize='True'):\n",
        "        s = env.reset()\n",
        "        total_reward = 0.0\n",
        "        finish = False\n",
        "\n",
        "        while not finish:\n",
        "            if visualize:\n",
        "                env.render()\n",
        "            q_vals = self.online_net.predict(np.array([s]))\n",
        "            a = np.argmax(q_vals)\n",
        "            s, r, finish, _ = env.step(a)\n",
        "            total_reward += reward\n",
        "        print(\"Total reward in the test: %.2f\" % total_reward)\n",
        "        env.close()"
      ],
      "metadata": {
        "id": "XwAoOQ3CBz-a"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base Loop** - train the model\n",
        "The loop(training) will continue as long as the model averages at least 475 operations until it drops."
      ],
      "metadata": {
        "id": "oF5XtFozG121"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "# Parameters\n",
        "env_dim = {\n",
        "        'state_dim': env.observation_space.shape[0],\n",
        "        'action_dim': env.action_space.n}\n",
        "\n",
        "# Hyper parameters\n",
        "hp = {\n",
        "        'lr': 0.0001,\n",
        "        'batch_size': 128,\n",
        "        'capacity': 10000,\n",
        "        'gamma': 0.99,\n",
        "        'max_epsilon': 0.9,\n",
        "        'min_epsilon': 0.01,\n",
        "        'epsilon_decay': 0.999,\n",
        "        'target_update_period': 100}\n",
        "\n",
        "agent = DQN_Agent(hp=hp, net_arch=env_dim)  # Build an agent object\n",
        "\n",
        "max_episodes = 1000\n",
        "max_steps = 500\n",
        "max_score = 475.0\n",
        "total_steps = 0\n",
        "episode_loss = []\n",
        "average_score = []\n",
        "for episode in range(max_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_score = 0\n",
        "    for step in range(max_steps):\n",
        "        b = True\n",
        "        total_steps += 1\n",
        "        action = agent.choose_action(observation=state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_score += reward\n",
        "\n",
        "        agent.store_experience((state, action, next_state, reward, not done))\n",
        "        if done:\n",
        "            b = False\n",
        "            average_score.append(episode_score)\n",
        "            print(\"Episode: {} | Score: {}\".format(episode + 1, episode_score))\n",
        "            break\n",
        "\n",
        "        state = next_state\n",
        "        loss = agent.update_step()\n",
        "        episode_loss.append(loss)\n",
        "\n",
        "        if (total_steps + 1) % hp['target_update_period'] == 0:\n",
        "          agent.update_target_network()\n",
        "    \n",
        "    if b:\n",
        "      print(\"Episode: {} | Score: {}\".format(episode + 1, episode_score))\n",
        "\n",
        "    if np.mean(average_score[-100:]) >= max_score:\n",
        "        print(\n",
        "            \"\\nGreat!! \"\n",
        "            \"You win after: {} Episodes\\n\"\n",
        "            \"Average reward in the last 100 episodes: {}\".format(episode + 1, np.mean(average_score[-100:])))\n",
        "        break\n",
        "        \n",
        "# save the target net\n",
        "agent.target_net.save('my_model1.h5')"
      ],
      "metadata": {
        "id": "fZgwGEavH5ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "f-c0l56h5DI6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('my_model.h5')"
      ],
      "metadata": {
        "id": "v9nd06BTBVPm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make('CartPole-v1'))\n",
        "s = env.reset()\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "    # The agent\n",
        "    q_vals = new_model.predict(np.array([s])) \n",
        "    a = np.argmax(q_vals)\n",
        "    s, r, finish, _ = env.step(a)\n",
        "     \n",
        "    if finish: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "1FL1flMj5Oz7",
        "outputId": "74024c8e-9d29-4373-8ce7-f7ce9d26bb6b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}